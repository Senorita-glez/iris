%%% Template para anotações de aula
%%% Feito por Daniel Campos com base no template de Willian Chamma que fez com base no template de  Mikhail Klassen



\documentclass[12pt,a4paper, brazil]{article}

%%%%%%% INFORMAÇÕES DO CABEÇALHO
\usepackage[spanish]{babel}
\newcommand{\userName}{IPN}
\newcommand{\institution}{IPN - UPIIT}
\usepackage{researchdiary_png}
\usepackage[style=abnt]{biblatex}
\addbibresource{referencias.bib}


\begin{document}

\begin{center}
{\textbf {\huge Investigación unidad 4}}\\[5mm]
{\large Daphne S. Gonzalez C.} \\[5mm]
\today\\[5mm] %% se quiser colocar data
\end{center}


\section{Aprendizaje}
The main practical objectives of machine learning consist of generating accurate predictions for unseen items and of designing efficient and robust algorithms to produce these predictions, even for large-scale problems. \textcite{Mohri2018-lf,}
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.5\textwidth]{Captura de pantalla 2023-06-01 232041.png}
        \caption{Illustration of the typical stages of a learning process.}
        \label{fig:exemplo}
    \end{figure}
\subsection{Modelo de aprendizaje supervisado}
The learner receives a set of labeled examples as training data and makes predictions for all unseen points. This is the most common scenario associated with classification, regression, and ranking problems. The spam detection problem discussed in the previous section is an instance of supervised learning.

\subsection{Modelo de aprendizaje no supervisado}
The learner exclusively receives unlabeled training data, and makes predictions for all unseen points. Since in general no labeled example is available in that setting, it can be difficult to quantitatively evaluate the performance of a learner. Clustering and dimensionality reduction are example of unsupervised learning problems

\section{Características de un conjunto de datos}
    \subsection{Tipos de características}
    
    \begin{description}
  \item[Data set type:] The type of data set being used can impact the choice of classification algorithm. For example, cybersecurity data sets may require different algorithms than remotely sensed image data sets.
  
  \item[Data set size:] The size of the data set can impact the performance of the classification algorithm. A larger data set may require more computational power and time to process.
  
  \item[Data set complexity:] The complexity of the data set can impact the accuracy of the classification algorithm. More complex data sets may require more sophisticated algorithms to accurately classify the data.
  
  \item[Data set distribution:] The distribution of the data set can impact the performance of the classification algorithm. A well-distributed data set can help ensure that the algorithm is trained on a representative sample of the data.
  
  \item[Data set quality:] The quality of the data set can impact the accuracy of the classification algorithm. A data set with errors or inconsistencies may lead to inaccurate classification results.
  
  \item[Data set features:] The features of the data set can impact the choice of classification algorithm. Different algorithms may be better suited to handle certain types of features, such as numerical or categorical data.
\end{description} \cite{8947945}

    \subsection{Problemas en los conjuntos de datos}
        \subsubsection{Tamaño de la muestra pequeño}
        
    
\begin{description}
  \item[Limited training data:] A small dataset may not provide enough data to train a classification algorithm effectively. This can lead to overfitting or underfitting of the data.
  
  \item[Feature selection:] Feature selection becomes more important when working with a small dataset. It is important to choose the most relevant features to avoid misleading the classification algorithm.
  
  \item[Choice of algorithm:] The choice of classification algorithm becomes more important when working with a small dataset. Some algorithms may be better suited to handle small datasets than others.
  
  \item[Validation:] Validation of the classification algorithm becomes more important when working with a small dataset. It is important to use techniques such as cross-validation to ensure that the algorithm is not overfitting the data.
\end{description}

        \subsubsection{Imbalance de clases}
        Class imbalance in a dataset occurs when the number of instances in one class is significantly more than the number of instances in another class. This can lead to biased classification results, where the algorithm may favor the majority class and ignore the minority class. Here are some ways to deal with class imbalance in a dataset:

    \begin{description}
  \item[Resampling:] Resampling techniques can be used to balance the class distribution in the dataset. This can involve oversampling the minority class, undersampling the majority class, or a combination of both.
  
  \item[Cost-sensitive learning:] Cost-sensitive learning involves assigning different costs to different types of classification errors. This can help the algorithm to focus on correctly classifying the minority class, which may be more important in certain applications.
  
  \item[Ensemble methods:] Ensemble methods involve combining multiple classifiers to improve classification performance. This can be particularly effective when dealing with imbalanced datasets, as it can help to balance the class distribution and reduce the impact of noisy data.
  
  \item[Algorithm selection:] Some classification algorithms are better suited to handle imbalanced datasets than others. For example, decision trees and support vector machines have been shown to perform well on imbalanced datasets.
\end{description}

        \subsubsection{Complejidad}
        A complex dataset can present several problems when it comes to classification and machine learning. Here are some of the issues that can arise:

    \begin{description}
  \item[High dimensionality:] A complex dataset may have a large number of features, which can make it difficult to analyze and classify. High dimensionality can lead to the curse of dimensionality, where the data becomes sparse and the classification algorithm may struggle to find meaningful patterns.
  
  \item[Class imbalance:] A complex dataset may have an uneven distribution of classes, where one class is significantly more common than the others. This can lead to biased classification results, where the algorithm may favor the majority class and ignore the minority class.
  
  \item[Computational complexity:] A complex dataset may require significant computational resources to analyze and classify. This can lead to long processing times and may require specialized hardware or software.
  
  \item[Noise and outliers:] A complex dataset may contain noise or outliers, which can make it difficult to identify meaningful patterns and classify the data accurately. Outliers can also skew the results of the classification algorithm.
  
  \item[Interactions between features:] A complex dataset may have complex interactions between features, where the relationship between features is not straightforward. This can make it difficult to identify meaningful patterns and may require more sophisticated classification algorithms.
\end{description}
    
        \subsubsection{Cambio del conjunto de datos}
Changes in a dataset can present several problems when it comes to classification and machine learning. Here are some of the issues that can arise:

    
\begin{description}
  \item[Class imbalance:] Changes in a dataset can lead to an uneven distribution of classes, where one class is significantly more common than the others. This can lead to biased classification results, where the algorithm may favor the majority class and ignore the minority class.
  
  \item[Feature selection:] Changes in a dataset can lead to a large number of features, which can make it difficult to analyze and classify. It is important to carefully select the most relevant features to avoid misleading the classification algorithm.
  
  \item[Computational complexity:] Changes in a dataset can require significant computational resources to analyze and classify. This can lead to long processing times and may require specialized hardware or software.
  
  \item[Noise and outliers:] Changes in a dataset can introduce noise or outliers, which can make it difficult to identify meaningful patterns and classify the data accurately. Outliers can also skew the results of the classification algorithm.
  
  \item[Overfitting:] Changes in a dataset can lead to overfitting, where the classification algorithm becomes too closely tuned to the training data and does not generalize well to new data. This can lead to poor classification performance on new data.
\end{description}
        
        \subsubsection{Datos ruidosos}
        Noisy data in a dataset refers to data that contains errors, outliers, or other inconsistencies that can affect the accuracy of classification and machine learning algorithms. Here are some reasons why noisy data can occur in a dataset:

    \begin{description}
  \item[Measurement errors:] Non-typical data can occur due to measurement errors, such as sensor malfunctions or data entry mistakes.

  \item[Outliers:] Non-typical data can occur due to outliers, which are data points that are significantly different from the other data points in the dataset. Outliers can be caused by measurement errors, data entry mistakes, or other factors.

  \item[Sampling errors:] Non-typical data can occur due to sampling errors, where the sample of data points is not representative of the population. This can be caused by biased sampling methods or small sample sizes.
\end{description}


    Data cleaning: Data cleaning involves identifying and removing noisy data from the dataset. This can be done manually or using automated techniques such as clustering or outlier detection.
    Data imputation: Data imputation involves filling in missing data points with estimated values based on the other data points in the dataset. This can be done using various techniques such as mean imputation, regression imputation, or k-nearest neighbor imputation.
    Feature selection: Feature selection involves identifying the most relevant features in the dataset and removing irrelevant or noisy features. This can improve the accuracy of classification algorithms and reduce the impact of noisy data.
    Ensemble methods: Ensemble methods involve combining multiple classifiers to improve classification performance. This can be particularly effective when dealing with noisy datasets, as it can help to reduce the impact of noisy data and improve classification accuracy.

        \subsubsection{Valores atípicos}
Non-typical data in a dataset refers to data that is significantly different from the other data points in the dataset. Here are some reasons why non-typical data can occur in a dataset:

        \begin{description}
      \item[Measurement errors:] Non-typical data can occur due to measurement errors, such as sensor malfunctions or data entry mistakes.
    
      \item[Outliers:] Non-typical data can occur due to outliers, which are data points that are significantly different from the other data points in the dataset. Outliers can be caused by measurement errors, data entry mistakes, or other factors.
    
      \item[Sampling errors:] Non-typical data can occur due to sampling errors, where the sample of data points is not representative of the population. This can be caused by biased sampling methods or small sample sizes.
    \end{description}
        
    \subsection{Selección de características}
        \subsubsection{Relevancia}
        Feature selection is the process of selecting the most relevant features in a dataset for a particular classification task. Here are some techniques for feature selection:

            \begin{description}
          \item[Filter methods:] Filter methods involve selecting features based on statistical measures such as correlation, mutual information, or chi-squared test. These methods are computationally efficient and can be used as a preprocessing step before applying a classification algorithm.
        
          \item[Wrapper methods:] Wrapper methods involve selecting features by evaluating the performance of a classification algorithm with different subsets of features. These methods are computationally expensive but can lead to better classification accuracy.
        
          \item[Embedded methods:] Embedded methods involve selecting features as part of the classification algorithm itself. These methods are computationally efficient and can lead to better classification accuracy.
        \end{description}

        \subsubsection{Redundancia}
        To avoid redundancy in a selection of a dataset, feature selection techniques can be used to select the most relevant features for a particular classification task. Here are some techniques for avoiding redundancy:

            \begin{description}
          \item[Filter methods:] Filter methods involve selecting features based on statistical measures such as correlation, mutual information, or chi-squared test. These methods can help to identify redundant features that are highly correlated with other features in the dataset.
        
          \item[Wrapper methods:] Wrapper methods involve selecting features by evaluating the performance of a classification algorithm with different subsets of features. These methods can help to identify redundant features that do not contribute significantly to the classification accuracy.
        
          \item[Embedded methods:] Embedded methods involve selecting features as part of the classification algorithm itself. These methods can help to identify redundant features that are not necessary for the classification task.
        \end{description}

        \subsubsection{Métodos de selección}
        
            \begin{description}
          \item[Filter methods:] Filter methods involve selecting features based on statistical measures such as correlation, mutual information, or chi-squared test. These methods can help to identify relevant features that are highly correlated with the target variable.
        
          \item[Wrapper methods:] Wrapper methods involve selecting features by evaluating the performance of a classification algorithm with different subsets of features. These methods can help to identify relevant features that contribute significantly to the classification accuracy.
        
          \item[Embedded methods:] Embedded methods involve selecting features as part of the classification algorithm itself. These methods can help to identify relevant features that are necessary for the classification task.
        
          \item[Dimensionality reduction:] Dimensionality reduction techniques such as principal component analysis (PCA) can be used to reduce the number of features in a dataset while preserving the most important information.
        
          \item[Sampling techniques:] Sampling techniques such as oversampling or undersampling can be used to balance the dataset and improve the performance of feature selection algorithms.
        \end{description}


\section{Algoritmos basados en distancia}
    \subsection{Clasificación usando KNN y Clasificador mínima distancia}
    
\begin{itemize}
  \item The KNN algorithm is a type of instance-based learning, which means that it does not explicitly learn a model. Instead, it stores the training instances and uses them to classify new instances.
  
  \item The KNN algorithm works by finding the K nearest neighbors to a new instance in the training set, based on a distance metric such as Euclidean distance. The class of the new instance is then assigned based on the majority class of its K nearest neighbors.
  
  \item The value of K is a hyperparameter that can be tuned to optimize the classification accuracy of the algorithm.
  
  \item The KNN algorithm can be used for both binary and multiclass classification tasks.
  
  \item The KNN algorithm does not require any assumptions about the underlying distribution of the data, making it a non-parametric algorithm.
  
  \item The KNN algorithm can be sensitive to the choice of distance metric and the scaling of the features.
\end{itemize}

    \subsection{Agrupamiento por K-Medias}
    K-means clustering is a popular unsupervised machine learning algorithm used for clustering similar data points together based on their similarities.
    \begin{itemize}
  \item K-means clustering is an unsupervised learning algorithm, which means that it does not require labeled data to train a model.
  
  \item The algorithm works by partitioning a dataset into K clusters, where K is a hyperparameter that needs to be specified by the user.
  
  \item The algorithm starts by randomly initializing K cluster centroids, and then iteratively assigns each data point to the nearest centroid and updates the centroid based on the mean of the data points assigned to it.
  
  \item The algorithm continues to iterate until the centroids no longer move significantly or a maximum number of iterations is reached.
  
  \item The quality of the clustering is measured by the within-cluster sum of squared distances, which should be minimized.
  
  \item The K-means algorithm can be sensitive to the initial placement of the cluster centroids, which can lead to different results for different initializations.
  
  \item The K-means algorithm can be used for a variety of applications, such as image segmentation, customer segmentation, and anomaly detection.
\end{itemize}
    
\section{Árboles de decisión}
    \subsection{Representación}
    \subsection{Algoritmos id3 y C4.5}

\section{Métodos de validación}
    \subsection{Entrenamiento y prueba}
    \subsection{Validación cruzada}
    \subsection{Matriz de confusión}

O que foi pedido para fazer? Crie uma lista/tabela dos itens que deveriam ser feitos. Aqui é uma espécie de objetivos.

\section{Métodos e Técnicas}

Aqui você deve explicar que técnicas foram usadas, de forma teórica e prática. Por exemplo, usou Fourier? Qual é o conceito por trás disso? Quais são as funções que executam? Aqui você pode intercalar partes de código com teoria e também trazer fluxogramas/diagramas explicativos da forma como o problema foi resolvido. 

%% para citar no texto, use \textcite{citacao} para ficar no formato Fulano (Ano), ou use \cite{citacao} para citar  no formato (FULANO, Ano)
Segue alguns exemplos de como utilizar citações. Segundo \textcite{citacao-exemplo}, a teoria é importante para criar a prática \cite{citacao-exemplo}. As referências são colocadas no arquivo referencias.bib. 

\par
Aqui vai um exemplo de uma equação utilizando um enviroment (que podem ser usados para criar listas/tabelas/figuras etc): 

\begin{equation}
\label{eq:sub}
y[n] = x[Mn]
\end{equation}

onde $y$ é a saída do sistema, $x$ é a entrada e $M$ é um fator de subamostragem. Veja que é preciso utilizar \$ para escrever variáveis e equações ao longo do texto. É possível referenciar a figura se você criou um label no enviroment, então posso falar "Figura \ref{eq:sub}" sem realmente precisar numerar ela no texto. 



\subsection{Subseção dos métodos}

Também é possível fazer subseções. É recomendável para organizar o texto.

É possível inserir figuras também. Lembre-se sempre que é importante que a Figura seja OU uma figura própria OU deve ser referenciada. Se for utilizada de uma fonte externa, procure figuras de qualidade. Se for criar Figuras, utilize softwares apropriados (Visio, Corel Draw, Inkscape, etc); É importante que a figura esteja em formato vetorial (por exemplo, o LaTeX trabalha bem como figuras em formato pdf).


\par A Figura \ref{fig:exemplo} é uma imagem com o logo da UTFPR. O [!htb] indica a ordem se preferência de onde a figura está, ! tenta deixar a imagem no melhor lugar para o texto ficar organizado, h é here (na mesma posição), t é topo da página e b é bottom (a parte inferior). Na dúvida, se estiver ficando muito feio com o [!htb] coloca [t] que evita que a figura fique fora de lugar.

\begin{figure}[t]
    \centering
    \caption{Figura exemplo: logo da UTFPR.}
    \includegraphics[width=0.5\textwidth]{logo_utf.png}
    \caption*{Fonte: Autoria própria.} %% isso é levemente uma gambiarra, mas funciona para o propósito desse template.
    \label{fig:exemplo}
\end{figure}



\section{Resultados e Considerações}

Aqui você deve colocar todas as figuras resultantes, incluindo a discussão sobre o que foi observado e suas considerações; 

%%% as referências devem estar em formato bibTeX no arquivo referencias.bib
\printbibliography

\end{document}